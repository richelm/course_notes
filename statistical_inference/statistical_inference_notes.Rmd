---
title: "Statistical Inference Notes"
output: html_document
---

## Conditional Probability
 
$P(A|B) = \frac{P(A \cap B)}{P(B)}$

From the definition of $P(A|B)$, we can write $P(A \cap B) = P(A|B) * P(B)$. And use this to express $P(B|A)$.

$P(B|A) = P(B \cap A) / P(A) = P(A|B) * P(B) / P(A)$. 

This is a simple form of Bayes' Rule which relates the two conditional probabilities.

Suppose we don't know P(A) itself, but only know its conditional probabilities that is, the probability that it occurs if B occurs and the probability that it occurs if B doesn't occur. These are P(A|B) and P(A|~B), respectively. We use ~B to represent 'not B' or 'B complement'.

We can then express $P(A) = P(A|B) * P(B) + P(A|~B) * P(~B)$ and substitute this is into the denominator of Bayes' Formula.

$P(B|A) = \frac{P(A|B) * P(B)}{P(A|B) * P(B) + P(A|~B) * P(~B)}$

## Common Distributions

**Bernoulli**  
Two outcomes: 1 (success) and 0 (failure)  
$P(1) = p$ and $P(0) = 1 - p$  
PMF: $p^x * (1-p)^(1-x)$  
$E[X] = 1*p + 0*(1-p) = p$  
$E[X^2] = 1^2p + 0^2(1-p) = p$  
Var = $E[X^2] - ((E[X]))^2 = p - p^2 = p(1-p)$  

